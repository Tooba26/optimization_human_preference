{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "562ce6f6-51fb-4603-9e3a-cc093ba93dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the Dahoas/full-hh-rlhf dataset\n",
    "dataset = load_dataset(\"Dahoas/full-hh-rlhf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c438b51f-c280-40a8-a26d-b3963eaca00d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "# Set GPU device\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "\n",
    "os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
    "os.environ['https_proxy'] = 'http://192.41.170.23:3128'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad3def1e-94ea-4c7f-b71e-5d91312c2bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['prompt', 'response', 'chosen', 'rejected'],\n",
      "        num_rows: 112052\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['prompt', 'response', 'chosen', 'rejected'],\n",
      "        num_rows: 12451\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96f13b75-4a8e-49d0-8717-d66bdcbc7f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['prompt', 'response', 'chosen', 'rejected']\n",
      "{'prompt': '\\n\\nHuman: Should you buy a case to protect your cell phone?\\n\\nAssistant: It depends on your circumstances.  If you carry your phone in a pocket or a purse then you probably want a case.  But if you only need a phone for quick interactions, a case may actually cause more harm than good.  What do you need the phone for?  Are you a parent, or do you work from home?\\n\\nHuman: What harm could it do?\\n\\nAssistant: A phone case can damage the screen, for one thing.  It can also get you in trouble if you have your phone turned off for some reason.  Then you will turn it back on and it wonâ€™t do anything.  If you can afford to replace it, then you need a case to protect it.  The problem is that most people arenâ€™t able to afford to replace their phones all the time.\\n\\nHuman: Thanks for letting me know.\\n\\nAssistant:', 'response': ' Youâ€™re welcome.', 'chosen': ' Youâ€™re welcome.', 'rejected': ' It sounds like youâ€™ve got the basics down.  Any further questions or concerns?  You can send me any feedback you have at help@babba.ai.'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"].column_names)\n",
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6412d083-c1f7-49ee-ba6f-f3220c2f40b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(example):\n",
    "    return {\n",
    "        \"prompt\": example[\"prompt\"],\n",
    "        \"chosen\": example[\"chosen\"],\n",
    "        \"rejected\": example[\"rejected\"]\n",
    "    }\n",
    "\n",
    "# Apply preprocessing properly\n",
    "processed_dataset = dataset.map(preprocess_data, remove_columns=[\"response\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "959d0637-440e-45c9-9e1d-7798027efbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': '\\n\\nHuman: Should you buy a case to protect your cell phone?\\n\\nAssistant: It depends on your circumstances.  If you carry your phone in a pocket or a purse then you probably want a case.  But if you only need a phone for quick interactions, a case may actually cause more harm than good.  What do you need the phone for?  Are you a parent, or do you work from home?\\n\\nHuman: What harm could it do?\\n\\nAssistant: A phone case can damage the screen, for one thing.  It can also get you in trouble if you have your phone turned off for some reason.  Then you will turn it back on and it wonâ€™t do anything.  If you can afford to replace it, then you need a case to protect it.  The problem is that most people arenâ€™t able to afford to replace their phones all the time.\\n\\nHuman: Thanks for letting me know.\\n\\nAssistant:', 'response': ' Youâ€™re welcome.', 'chosen': ' Youâ€™re welcome.', 'rejected': ' It sounds like youâ€™ve got the basics down.  Any further questions or concerns?  You can send me any feedback you have at help@babba.ai.'}\n"
     ]
    }
   ],
   "source": [
    "print(processed_dataset[\"train\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "136b5faa-925f-40db-8df0-757fca942cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f63bade-4396-41c5-94bd-99a8b8e09bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    HfArgumentParser, \n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "from typing import Dict, Optional\n",
    "\n",
    "# Implements Direct Preference Optimization for fine-tuning models based on human preference data\n",
    "from trl import DPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a6b1317-d703-4ba9-94fb-8ff4d85df7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Model configuration\n",
    "model_name_or_path = \"gpt2\"\n",
    "ignore_bias_buffers = False\n",
    "\n",
    "# Load pre-trained GPT-2 model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
    "\n",
    "if ignore_bias_buffers:\n",
    "    # Ignore boolean bias buffers in torch distributed training\n",
    "    model._ddp_params_and_buffers_to_ignore = [\n",
    "        name for name, buffer in model.named_buffers() if buffer.dtype == torch.bool\n",
    "    ]\n",
    "\n",
    "# Load reference model for DPO training\n",
    "model_ref = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
    "\n",
    "# Load tokenizer and ensure padding token exists\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d82d2e69-87aa-445d-bac5-9e4bad99436d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def extract_anthropic_prompt(prompt_and_response):\n",
    "    \"\"\"Extract the anthropic prompt from a prompt and response pair.\"\"\"\n",
    "    search_term = \"\\n\\nAssistant:\"\n",
    "    search_term_idx = prompt_and_response.rfind(search_term)\n",
    "    if search_term_idx == -1:\n",
    "        return None  # Return None if no valid prompt structure found\n",
    "    return prompt_and_response[: search_term_idx + len(search_term)]\n",
    "\n",
    "def get_hh(split: str, sanity_check: bool = False, cache_dir: str = None):\n",
    "    \"\"\"Load the Anthropic Helpful-Harmless dataset from Hugging Face and convert it to the necessary format.\"\"\"\n",
    "    dataset = load_dataset(\"Dahoas/full-hh-rlhf\", split=split, cache_dir=cache_dir)\n",
    "\n",
    "    # Limit dataset size for quick testing\n",
    "    if sanity_check:\n",
    "        dataset = dataset.select(range(min(len(dataset), 1000)))\n",
    "\n",
    "    def split_prompt_and_responses(sample):\n",
    "        prompt = extract_anthropic_prompt(sample[\"chosen\"])\n",
    "        if prompt is None:\n",
    "            return None  # Skip invalid samples\n",
    "\n",
    "        return {\n",
    "            \"prompt\": prompt,\n",
    "            \"chosen\": sample[\"chosen\"][len(prompt) :].strip(),\n",
    "            \"rejected\": sample[\"rejected\"][len(prompt) :].strip(),\n",
    "        }\n",
    "\n",
    "    processed_dataset = dataset.map(split_prompt_and_responses, remove_columns=[\"chosen\", \"rejected\"])\n",
    "    \n",
    "    return processed_dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ecbb77c5-14af-4e55-af68-7b94cccc2773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "377f922016854a7e8beee387136d1c88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': '\\n\\nHuman: Should you buy a case to protect your cell phone?\\n\\nAssistant: It depends on your circumstances.  If you carry your phone in a pocket or a purse then you probably want a case.  But if you only need a phone for quick interactions, a case may actually cause more harm than good.  What do you need the phone for?  Are you a parent, or do you work from home?\\n\\nHuman: What harm could it do?\\n\\nAssistant: A phone case can damage the screen, for one thing.  It can also get you in trouble if you have your phone turned off for some reason.  Then you will turn it back on and it wonâ€™t do anything.  If you can afford to replace it, then you need a case to protect it.  The problem is that most people arenâ€™t able to afford to replace their phones all the time.\\n\\nHuman: Thanks for letting me know.\\n\\nAssistant:', 'response': ' Youâ€™re welcome.', 'chosen': ' Youâ€™re welcome.', 'rejected': ' It sounds like youâ€™ve got the basics down.  Any further questions or concerns?  You can send me any feedback you have at help@babba.ai.'}\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "processed_dataset = get_hh(\"train\", sanity_check=True)\n",
    "print(processed_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80454e29-8cb5-4ec3-b251-49107d296eca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b93422cb50f4f9eb392b97a4e9af133",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23fb515768c94febb49351099f956bd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sanity_check = True\n",
    "train_dataset = get_hh(\"train\", sanity_check=sanity_check)\n",
    "eval_dataset = get_hh(\"test\", sanity_check=sanity_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82896cb5-8830-428d-b84a-1142c5d19048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'response', 'chosen', 'rejected'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eac75592-5f10-4745-8068-f1a66344fd25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'response', 'chosen', 'rejected'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a31a9b1-febb-4ff0-aef0-9e1ce8b25bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 5e-5  \n",
    "per_device_train_batch_size = 8  \n",
    "gradient_accumulation_steps = 2  \n",
    "max_length = 512  \n",
    "max_prompt_length = 128  # Standard for DPO training\n",
    "max_target_length = 128  # Standard response length\n",
    "label_pad_token_id = -100  # For ignored token loss\n",
    "max_steps = 100  \n",
    "sanity_check = True \n",
    "report_to = \"wandb\" \n",
    "gradient_checkpointing = True  # Saves memory\n",
    "beta = 0.1  # Standard for DPO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eaf708d1-fd34-4ef1-9b95-9fd0e7e8980f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import DPOTrainer, DPOConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250768db-abf5-4990-ae9c-b0c98645be70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-st125404/.local/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "\n",
    "training_args = DPOConfig(\n",
    "    output_dir=\"./gpt2_dpo\",\n",
    "    per_device_train_batch_size=2,  \n",
    "    per_device_eval_batch_size=2,  \n",
    "    gradient_accumulation_steps=8, \n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=3,\n",
    "    max_steps=1000,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    fp16=True,  # Mixed Precision\n",
    "    gradient_checkpointing=True,  # Saves memory\n",
    "    padding_value=tokenizer.pad_token_id,\n",
    "    max_prompt_length=384,  \n",
    "    max_completion_length=384,  \n",
    ")\n",
    "\n",
    "\n",
    "# âœ… 4. Initialize DPOTrainer\n",
    "trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    processing_class=tokenizer  \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2aefe0ee-86d9-4ad2-a879-30c8b852eaa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'response', 'chosen', 'rejected'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2e12419-1421-44b2-ae5f-cc826d0be94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 27:27, Epoch 15/17]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rewards/chosen</th>\n",
       "      <th>Rewards/rejected</th>\n",
       "      <th>Rewards/accuracies</th>\n",
       "      <th>Rewards/margins</th>\n",
       "      <th>Logps/chosen</th>\n",
       "      <th>Logps/rejected</th>\n",
       "      <th>Logits/chosen</th>\n",
       "      <th>Logits/rejected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.793600</td>\n",
       "      <td>0.754662</td>\n",
       "      <td>-0.187384</td>\n",
       "      <td>-0.350300</td>\n",
       "      <td>0.559000</td>\n",
       "      <td>0.162916</td>\n",
       "      <td>-229.084061</td>\n",
       "      <td>-214.057678</td>\n",
       "      <td>-108.798828</td>\n",
       "      <td>-109.278664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.328000</td>\n",
       "      <td>0.845747</td>\n",
       "      <td>-0.649858</td>\n",
       "      <td>-0.906046</td>\n",
       "      <td>0.568000</td>\n",
       "      <td>0.256188</td>\n",
       "      <td>-233.708817</td>\n",
       "      <td>-219.615128</td>\n",
       "      <td>-111.854401</td>\n",
       "      <td>-112.525116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.123400</td>\n",
       "      <td>1.139908</td>\n",
       "      <td>-2.600197</td>\n",
       "      <td>-2.979456</td>\n",
       "      <td>0.582000</td>\n",
       "      <td>0.379259</td>\n",
       "      <td>-253.212219</td>\n",
       "      <td>-240.349228</td>\n",
       "      <td>-126.323257</td>\n",
       "      <td>-127.757065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.011600</td>\n",
       "      <td>1.207658</td>\n",
       "      <td>-3.487255</td>\n",
       "      <td>-3.968986</td>\n",
       "      <td>0.579000</td>\n",
       "      <td>0.481731</td>\n",
       "      <td>-262.082794</td>\n",
       "      <td>-250.244568</td>\n",
       "      <td>-120.031448</td>\n",
       "      <td>-120.944740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>1.588167</td>\n",
       "      <td>-5.523031</td>\n",
       "      <td>-6.025922</td>\n",
       "      <td>0.573000</td>\n",
       "      <td>0.502891</td>\n",
       "      <td>-282.440521</td>\n",
       "      <td>-270.813873</td>\n",
       "      <td>-124.085159</td>\n",
       "      <td>-125.178078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.007400</td>\n",
       "      <td>1.638352</td>\n",
       "      <td>-5.922364</td>\n",
       "      <td>-6.440679</td>\n",
       "      <td>0.566000</td>\n",
       "      <td>0.518315</td>\n",
       "      <td>-286.433868</td>\n",
       "      <td>-274.961456</td>\n",
       "      <td>-121.233566</td>\n",
       "      <td>-122.241165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.007200</td>\n",
       "      <td>1.651491</td>\n",
       "      <td>-6.012239</td>\n",
       "      <td>-6.533665</td>\n",
       "      <td>0.565000</td>\n",
       "      <td>0.521427</td>\n",
       "      <td>-287.332611</td>\n",
       "      <td>-275.891327</td>\n",
       "      <td>-117.790222</td>\n",
       "      <td>-118.703743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.006200</td>\n",
       "      <td>1.708938</td>\n",
       "      <td>-6.328568</td>\n",
       "      <td>-6.852242</td>\n",
       "      <td>0.565000</td>\n",
       "      <td>0.523674</td>\n",
       "      <td>-290.495911</td>\n",
       "      <td>-279.077087</td>\n",
       "      <td>-115.711937</td>\n",
       "      <td>-116.539482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.006200</td>\n",
       "      <td>1.732372</td>\n",
       "      <td>-6.474491</td>\n",
       "      <td>-7.001331</td>\n",
       "      <td>0.565000</td>\n",
       "      <td>0.526840</td>\n",
       "      <td>-291.955139</td>\n",
       "      <td>-280.567993</td>\n",
       "      <td>-113.815544</td>\n",
       "      <td>-114.587425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.006200</td>\n",
       "      <td>1.758846</td>\n",
       "      <td>-6.628852</td>\n",
       "      <td>-7.157539</td>\n",
       "      <td>0.567000</td>\n",
       "      <td>0.528687</td>\n",
       "      <td>-293.498749</td>\n",
       "      <td>-282.130096</td>\n",
       "      <td>-112.411179</td>\n",
       "      <td>-113.151146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.008700</td>\n",
       "      <td>1.771053</td>\n",
       "      <td>-6.710707</td>\n",
       "      <td>-7.241150</td>\n",
       "      <td>0.565000</td>\n",
       "      <td>0.530443</td>\n",
       "      <td>-294.317322</td>\n",
       "      <td>-282.966187</td>\n",
       "      <td>-111.207123</td>\n",
       "      <td>-111.925835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>1.781875</td>\n",
       "      <td>-6.774649</td>\n",
       "      <td>-7.304294</td>\n",
       "      <td>0.565000</td>\n",
       "      <td>0.529645</td>\n",
       "      <td>-294.956726</td>\n",
       "      <td>-283.597595</td>\n",
       "      <td>-110.181648</td>\n",
       "      <td>-110.880501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>1.792192</td>\n",
       "      <td>-6.841235</td>\n",
       "      <td>-7.372271</td>\n",
       "      <td>0.567000</td>\n",
       "      <td>0.531035</td>\n",
       "      <td>-295.622589</td>\n",
       "      <td>-284.277374</td>\n",
       "      <td>-109.508568</td>\n",
       "      <td>-110.201843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.009600</td>\n",
       "      <td>1.799677</td>\n",
       "      <td>-6.890253</td>\n",
       "      <td>-7.422534</td>\n",
       "      <td>0.568000</td>\n",
       "      <td>0.532282</td>\n",
       "      <td>-296.112762</td>\n",
       "      <td>-284.779999</td>\n",
       "      <td>-109.024948</td>\n",
       "      <td>-109.708382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>1.806840</td>\n",
       "      <td>-6.928459</td>\n",
       "      <td>-7.460698</td>\n",
       "      <td>0.563000</td>\n",
       "      <td>0.532239</td>\n",
       "      <td>-296.494812</td>\n",
       "      <td>-285.161682</td>\n",
       "      <td>-108.617012</td>\n",
       "      <td>-109.292000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=0.07009943208098411, metrics={'train_runtime': 1648.3889, 'train_samples_per_second': 9.706, 'train_steps_per_second': 0.607, 'total_flos': 0.0, 'train_loss': 0.07009943208098411, 'epoch': 15.88})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84a7f5aa-0f60-4ba0-8b75-ff73d2ce1ec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./gpt2_dpo_finetuned/tokenizer_config.json',\n",
       " './gpt2_dpo_finetuned/special_tokens_map.json',\n",
       " './gpt2_dpo_finetuned/vocab.json',\n",
       " './gpt2_dpo_finetuned/merges.txt',\n",
       " './gpt2_dpo_finetuned/added_tokens.json',\n",
       " './gpt2_dpo_finetuned/tokenizer.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model and tokenizer locally\n",
    "output_dir = \"./gpt2_dpo_finetuned\"\n",
    "\n",
    "trainer.save_model(output_dir)  # Saves the model\n",
    "tokenizer.save_pretrained(output_dir)  # Saves the tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4152ac98-76d7-48ad-b49e-57296160d067",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
